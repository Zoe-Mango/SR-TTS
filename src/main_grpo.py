import json
from pathlib import Path
import random
import re
import csv
from typing import Iterator, Optional
import subprocess
import sys
import os
import uuid
import tempfile
import numpy as np

import wandb
import torch
import torch.optim as optim
import torch.nn.functional as F
from torch.nn.utils import clip_grad_norm_
from torch.utils.data import DataLoader
from peft import get_peft_model, LoraConfig, TaskType, PeftModel
from transformers import (
    AutoTokenizer,
    PreTrainedTokenizer,
    GenerationConfig,
)
from MiniCPM_o.modeling_minicpmo import MiniCPMO
from loss import approx_kl_divergence, GRPOLoss
from replay_buffer import ReplayBuffer, Experience, join_experience_batch

import torchaudio
import jiwer

from transformers import WhisperProcessor, WhisperForConditionalGeneration
import librosa


# è¾“å‡ºè·¯å¾„é…ç½®
OUTPUT_BASE = "output path"
AUDIO_OUTPUT_DIR = os.path.join(OUTPUT_BASE, "audio")
WER_RESULT_CSV = os.path.join(OUTPUT_BASE, "wer_result.csv")
AUDIO_LABEL_RESULT_JSON = os.path.join(OUTPUT_BASE, "audio_label_result.json")

# åˆ›å»ºè¾“å‡ºç›®å½•
os.makedirs(AUDIO_OUTPUT_DIR, exist_ok=True)
os.makedirs(OUTPUT_BASE, exist_ok=True)

# è·¨ç¯å¢ƒé…ç½®
QWEN_AUDIO_ENV_PATH = "envoronment/qwen_audio/bin/python" 
QWEN_AUDIO_SCRIPT_PATH = "the path of qwen_audio_service.py"       

 
class OrderedDataIterator:
    """æœ‰åºæ•°æ®è¿­ä»£å™¨ï¼Œæ”¯æŒæ–­ç‚¹ç»­ä¼ """
    
    def __init__(self, data_list, start_index=0):
        self.data_list = data_list
        self.current_index = start_index
        self.total_length = len(data_list)
        
    def __iter__(self):
        return self
    
    def __next__(self):
        if self.current_index >= self.total_length:
            raise StopIteration
        
        item = self.data_list[self.current_index]
        self.current_index += 1
        return item, self.current_index - 1  # è¿”å›æ•°æ®å’Œç´¢å¼•
    
    def __len__(self):
        return self.total_length - self.current_index
    
    def get_current_index(self):
        return self.current_index
    
    def set_current_index(self, index):
        self.current_index = max(0, min(index, self.total_length))
        
    def reset(self):
        self.current_index = 0
        
    def is_finished(self):
        return self.current_index >= self.total_length


class QwenAudioAnalyzer:
    """Qwen Audio åˆ†æå™¨ - è·¨ç¯å¢ƒè°ƒç”¨ç‰ˆæœ¬"""
    
    def __init__(self):
        self._service_ready = False
        self._warmup_done = False
        
    def _ensure_service_ready(self):
        """ç¡®ä¿æœåŠ¡è¿›ç¨‹å‡†å¤‡å°±ç»ª - é¢„çƒ­æœåŠ¡"""
        if not self._warmup_done:
            try:
                print("[INFO] é¢„çƒ­ Qwen Audio æœåŠ¡...")
                # åˆ›å»ºä¸€ä¸ªä¸´æ—¶çš„å°éŸ³é¢‘æ–‡ä»¶è¿›è¡Œæµ‹è¯•
                with tempfile.NamedTemporaryFile(suffix='.wav', delete=False) as tmp_file:
                    # åˆ›å»º1ç§’çš„é™éŸ³éŸ³é¢‘
                    import numpy as np
                    import soundfile as sf
                    
                    dummy_audio = np.zeros(16000, dtype=np.float32)
                    sf.write(tmp_file.name, dummy_audio, 16000)
                    
                    # é¢„çƒ­è°ƒç”¨
                    cmd = [QWEN_AUDIO_ENV_PATH, QWEN_AUDIO_SCRIPT_PATH, tmp_file.name]
                    result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)
                    
                    # æ¸…ç†ä¸´æ—¶æ–‡ä»¶
                    os.unlink(tmp_file.name)
                    
                    if result.returncode == 0:
                        self._warmup_done = True
                        print("[INFO] Qwen AudioæœåŠ¡é¢„çƒ­å®Œæˆ")
                    else:
                        print(f"[WARNING] Qwen AudioæœåŠ¡é¢„çƒ­å¤±è´¥: {result.stderr}")
                        
            except Exception as e:
                print(f"[WARNING] æœåŠ¡é¢„çƒ­å¤±è´¥: {e}")
    
    def analyze_audio_tags(self, audio_path: str) -> dict:
        """é€šè¿‡subprocessè°ƒç”¨qwen_audioç¯å¢ƒåˆ†æéŸ³é¢‘æ ‡ç­¾"""
        try:
            # ç¡®ä¿æœåŠ¡é¢„çƒ­
            self._ensure_service_ready()
            
            # æ£€æŸ¥éŸ³é¢‘æ–‡ä»¶æ˜¯å¦å­˜åœ¨
            if not os.path.exists(audio_path):
                print(f"[ERROR] éŸ³é¢‘æ–‡ä»¶ä¸å­˜åœ¨: {audio_path}")
                return {
                    "structure": "-1",
                    "emotion": "-1", 
                    "speech_speed": "-1",
                    "tone": "-1"
                }
                
            # æ£€æŸ¥æ–‡ä»¶å¤§å°
            if os.path.getsize(audio_path) == 0:
                print(f"[ERROR] éŸ³é¢‘æ–‡ä»¶ä¸ºç©º: {audio_path}")
                return {
                    "structure": "-1",
                    "emotion": "-1", 
                    "speech_speed": "-1",
                    "tone": "-1"
                }
            
            # è°ƒç”¨qwen_audioç¯å¢ƒä¸­çš„è„šæœ¬
            cmd = [QWEN_AUDIO_ENV_PATH, QWEN_AUDIO_SCRIPT_PATH, audio_path]
            print(f"[DEBUG] æ‰§è¡Œå‘½ä»¤: {' '.join(cmd)}")
            
            result = subprocess.run(cmd, capture_output=True, text=True, timeout=300)  # å¢åŠ è¶…æ—¶æ—¶é—´
            
            print(f"[DEBUG] è¿”å›ç : {result.returncode}")
            
            if result.returncode == 0:
                # è§£æJSONè¾“å‡º
                stdout_lines = result.stdout.strip().split('\n')
                json_line = None
                
                # æŸ¥æ‰¾JSONè¡Œï¼ˆé€šå¸¸æ˜¯æœ€åä¸€è¡Œï¼‰
                for line in reversed(stdout_lines):
                    line = line.strip()
                    if line.startswith('{') and line.endswith('}'):
                        json_line = line
                        break
                
                if json_line:
                    try:
                        analysis_result = json.loads(json_line)
                        print(f"[DEBUG] æˆåŠŸè§£æJSON: {analysis_result}")
                        return analysis_result
                    except json.JSONDecodeError as e:
                        print(f"[ERROR] JSONè§£æå¤±è´¥: {e}")
                        print(f"[DEBUG] å°è¯•è§£æçš„å†…å®¹: {json_line}")
                else:
                    print("[ERROR] æœªæ‰¾åˆ°æœ‰æ•ˆçš„JSONè¾“å‡º")
                    if result.stdout:
                        print(f"[DEBUG] å®Œæ•´stdout: {result.stdout}")
                    
                # å¦‚æœæ²¡æœ‰æ‰¾åˆ°JSONï¼Œè¿”å›é»˜è®¤å€¼
                return {
                    "structure": "-1",
                    "emotion": "-1", 
                    "speech_speed": "-1",
                    "tone": "-1"
                }
            else:
                print(f"[ERROR] Qwen Audioåˆ†æå¤±è´¥ï¼Œè¿”å›ç : {result.returncode}")
                if result.stderr:
                    print(f"[ERROR] é”™è¯¯ä¿¡æ¯: {result.stderr}")
                return {
                    "structure": "-1",
                    "emotion": "-1", 
                    "speech_speed": "-1",
                    "tone": "-1"
                }
                
        except subprocess.TimeoutExpired:
            print(f"[ERROR] Qwen Audioåˆ†æè¶…æ—¶: {audio_path}")
            return {
                "structure": "-1",
                "emotion": "-1", 
                "speech_speed": "-1",
                "tone": "-1"
            }
        except Exception as e:
            print(f"[ERROR] éŸ³é¢‘æ ‡ç­¾åˆ†æå¤±è´¥ {audio_path}: {e}")
            import traceback
            traceback.print_exc()
            return {
                "structure": "-1",
                "emotion": "-1", 
                "speech_speed": "-1",
                "tone": "-1"
            }
    
    def compute_tag_match_reward(self, audio_path: str, target_labels: dict) -> float:
        """è®¡ç®—æ ‡ç­¾åŒ¹é…å¥–åŠ±"""
        try:
            # åˆ†æéŸ³é¢‘æ ‡ç­¾
            pred_labels = self.analyze_audio_tags(audio_path)
            
            # è®¡ç®—åŒ¹é…åº¦
            matches = 0
            total_labels = 4
            
            # æ ‡å‡†åŒ–æ¯”è¾ƒ
            for key in ["structure", "emotion", "speech_speed", "tone"]:
                target_value = str(target_labels.get(key, "")).lower().strip()
                pred_value = str(pred_labels.get(key, "")).lower().strip()
                
                # å¤„ç†ç‰¹æ®Šæƒ…å†µ
                if key == "speech_speed":
                    # å¤„ç†è¯­é€Ÿæ ‡ç­¾çš„ä¸åŒè¡¨ç¤ºæ–¹å¼
                    target_value = target_value.replace(" ", "_")
                    pred_value = pred_value.replace(" ", "_")
                
                print(f"[DEBUG] æ ‡ç­¾æ¯”è¾ƒ - {key}: target='{target_value}', predicted='{pred_value}'")
                
                # å¦‚æœé¢„æµ‹å€¼ä¸æ˜¯-1ä¸”ä¸ç›®æ ‡å€¼åŒ¹é…ï¼Œåˆ™ç®—ä½œåŒ¹é…
                if target_value and pred_value != "-1" and pred_value == target_value:
                    matches += 1
                    print(f"[DEBUG] âœ“ {key} åŒ¹é…æˆåŠŸ")
                else:
                    print(f"[DEBUG] âœ— {key} åŒ¹é…å¤±è´¥")
            
            reward = matches / total_labels if total_labels > 0 else 0.0
            
            # ä¿å­˜åˆ†æç»“æœ
            result_data = {
                "audio_path": audio_path,
                "predicted_labels": pred_labels,
                "target_labels": target_labels,
                "matches": matches,
                "total": total_labels,
                "reward": reward,
                "timestamp": str(torch.cuda.current_stream().query()) if torch.cuda.is_available() else "0"
            }
            
            # è¿½åŠ åˆ°JSONæ–‡ä»¶
            self._save_audio_label_result(result_data)
            
            print(f"[INFO] æ ‡ç­¾åŒ¹é…åˆ†æ - éŸ³é¢‘: {os.path.basename(audio_path)}")
            print(f"[INFO] åŒ¹é…ç»“æœ: {matches}/{total_labels}, å¥–åŠ±: {reward:.3f}")
            print(f"[INFO] é¢„æµ‹æ ‡ç­¾: {pred_labels}")
            print(f"[INFO] ç›®æ ‡æ ‡ç­¾: {target_labels}")
            
            return reward
            
        except Exception as e:
            print(f"[ERROR] æ ‡ç­¾åŒ¹é…è®¡ç®—å¤±è´¥: {e}")
            import traceback
            traceback.print_exc()
            return 0.0
    
    def _save_audio_label_result(self, result_data):
        """ä¿å­˜éŸ³é¢‘æ ‡ç­¾åˆ†æç»“æœ"""
        try:
            # è¯»å–ç°æœ‰æ•°æ®
            if os.path.exists(AUDIO_LABEL_RESULT_JSON):
                with open(AUDIO_LABEL_RESULT_JSON, 'r', encoding='utf-8') as f:
                    try:
                        existing_data = json.load(f)
                    except json.JSONDecodeError:
                        existing_data = []
            else:
                existing_data = []
            
            # æ·»åŠ æ–°æ•°æ®
            existing_data.append(result_data)
            
            # å†™å›æ–‡ä»¶
            with open(AUDIO_LABEL_RESULT_JSON, 'w', encoding='utf-8') as f:
                json.dump(existing_data, f, indent=2, ensure_ascii=False)
                
        except Exception as e:
            print(f"[ERROR] ä¿å­˜éŸ³é¢‘æ ‡ç­¾ç»“æœå¤±è´¥: {e}")


class WERCalculator:
    """WERè®¡ç®—å™¨"""
    
    def __init__(self):
        self.transformation = self._get_transformation()
        # åˆå§‹åŒ–CSVæ–‡ä»¶
        self._init_wer_csv()
    
    def _init_wer_csv(self):
        """åˆå§‹åŒ–WERç»“æœCSVæ–‡ä»¶"""
        if not os.path.exists(WER_RESULT_CSV):
            with open(WER_RESULT_CSV, 'w', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow(['id', 'ori_text', 'gen_text', 'ori_clean', 'gen_clean', 'wer'])
    
    def _get_transformation(self):
        """è·å–æ–‡æœ¬æ ‡å‡†åŒ–è½¬æ¢å™¨"""
        _simple_number_map = {
            "zero": 0, "one": 1, "two": 2, "three": 3, "four": 4,
            "five": 5, "six": 6, "seven": 7, "eight": 8, "nine": 9,
            "ten": 10, "eleven": 11, "twelve": 12, "thirteen": 13,
            "fourteen": 14, "fifteen": 15, "sixteen": 16,
            "seventeen": 17, "eighteen": 18, "nineteen": 19,
            "twenty": 20, "thirty": 30, "forty": 40,
            "fifty": 50, "sixty": 60, "seventy": 70,
            "eighty": 80, "ninety": 90
        }
        
        def simple_text2num(phrase):
            words = phrase.lower().strip().split()
            total = 0
            for word in words:
                if word in _simple_number_map:
                    total += _simple_number_map[word]
            return str(total) if total > 0 else phrase
        
        def normalize_numbers(text):
            text = text.lower()
            pattern = r'\b(?:zero|one|two|three|four|five|six|seven|eight|nine|ten|' \
                    r'eleven|twelve|thirteen|fourteen|fifteen|sixteen|seventeen|' \
                    r'eighteen|nineteen|twenty|thirty|forty|fifty|sixty|seventy|' \
                    r'eighty|ninety)(?:\s(?:one|two|three|four|five|six|seven|eight|nine))?\b'
            return re.sub(pattern, lambda m: simple_text2num(m.group()), text)
        
        transformation = jiwer.Compose([
            jiwer.ToLowerCase(),
            normalize_numbers,
            jiwer.RemovePunctuation(),
            jiwer.RemoveMultipleSpaces(),
            jiwer.Strip()
        ])
        return transformation
    
    def transcribe_with_whisper(self, audio_path: str, processor, model, device="cuda:4"):
        """ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘"""
        try:
            print(f"[DEBUG] Whisperè½¬å½•éŸ³é¢‘: {audio_path}")
            audio, sr = librosa.load(audio_path, sr=16000)
            input_features = processor(audio, sampling_rate=16000, return_tensors="pt").input_features.to(device)
            with torch.no_grad():
                predicted_ids = model.generate(input_features)
            transcription = processor.batch_decode(predicted_ids, skip_special_tokens=True)[0]
            print(f"[DEBUG] Whisperè½¬å½•ç»“æœ: {transcription}")
            return transcription.strip()
        except Exception as e:
            print(f"[ERROR] Whisperè½¬å½•å¤±è´¥ {audio_path}: {e}")
            return ""
    
    def compute_wer(self, id: str, ori_text: str, gen_text: str) -> float:
        """è®¡ç®—WERå¹¶ä¿å­˜ç»“æœ"""
        try:
            # æ ‡å‡†åŒ–æ–‡æœ¬
            ori_clean = self.transformation(ori_text)
            gen_clean = self.transformation(gen_text)
            
            # è®¡ç®—WER
            wer = jiwer.wer(ori_clean, gen_clean)
            
            # ä¿å­˜åˆ°CSV
            with open(WER_RESULT_CSV, 'a', newline='', encoding='utf-8') as f:
                writer = csv.writer(f)
                writer.writerow([id, ori_text, gen_text, ori_clean, gen_clean, wer])
            
            print(f"[INFO] WERè®¡ç®— - ID: {id}, WER: {wer:.3f}")
            print(f"[INFO] åŸæ–‡: {ori_text}")
            print(f"[INFO] è¯†åˆ«: {gen_text}")
            print(f"[DEBUG] æ ‡å‡†åŒ–åŸæ–‡: {ori_clean}")
            print(f"[DEBUG] æ ‡å‡†åŒ–è¯†åˆ«: {gen_clean}")
            
            return wer
            
        except Exception as e:
            print(f"[ERROR] WERè®¡ç®—å¤±è´¥: {e}")
            return 1.0


def init_deterministic_rng(seed: int) -> torch.Generator:
    """åˆå§‹åŒ–ç¡®å®šæ€§éšæœºæ•°ç”Ÿæˆå™¨"""
    print(f"[INFO] è®¾ç½®ç¡®å®šæ€§éšæœºæ•°ç§å­: {seed}")
    
    # Pythonéšæœºæ•°
    random.seed(seed)
    
    # NumPyéšæœºæ•°
    np.random.seed(seed)
    
    # PyTorchéšæœºæ•°
    torch.manual_seed(seed)
    if torch.cuda.is_available():
        torch.cuda.manual_seed(seed)
        torch.cuda.manual_seed_all(seed)
    
    # è®¾ç½®ç¡®å®šæ€§è®¡ç®—
    torch.backends.cudnn.deterministic = True
    torch.backends.cudnn.benchmark = False
    
    return torch.manual_seed(seed)


def save_rng_state():
    """ä¿å­˜æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€"""
    return {
        'python_random_state': random.getstate(),
        'numpy_random_state': np.random.get_state(),
        'torch_random_state': torch.get_rng_state(),
        'torch_cuda_random_state': torch.cuda.get_rng_state_all() if torch.cuda.is_available() else None,
    }


def load_rng_state(rng_state):
    """æ¢å¤æ‰€æœ‰éšæœºæ•°ç”Ÿæˆå™¨çŠ¶æ€"""
    random.setstate(rng_state['python_random_state'])
    np.random.set_state(rng_state['numpy_random_state'])
    torch.set_rng_state(rng_state['torch_random_state'])
    if rng_state['torch_cuda_random_state'] is not None and torch.cuda.is_available():
        torch.cuda.set_rng_state_all(rng_state['torch_cuda_random_state'])


def save_checkpoint_complete(
    checkpoint_path: Path, 
    global_step: int, 
    epoch: int, 
    data_index: int,
    model, 
    tokenizer, 
    optimizer, 
    scheduler=None,
    replay_buffer=None,
    additional_state=None
):
    """å®Œæ•´çš„æ£€æŸ¥ç‚¹ä¿å­˜å‡½æ•°"""
    save_dir = checkpoint_path / f"step_{global_step}"
    save_dir.mkdir(parents=True, exist_ok=True)
    
    try:
        print(f"[INFO] ä¿å­˜æ£€æŸ¥ç‚¹åˆ°: {save_dir}")
        
        # 1. ä¿å­˜æ¨¡å‹å’Œtokenizer
        model.save_pretrained(save_dir)
        tokenizer.save_pretrained(save_dir)
        print("âœ… æ¨¡å‹å’Œtokenizerå·²ä¿å­˜")

        # 2. ä¿å­˜ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer_path = save_dir / "optimizer.pt"
        torch.save(optimizer.state_dict(), optimizer_path)
        print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²ä¿å­˜")
        
        # 3. ä¿å­˜å­¦ä¹ ç‡è°ƒåº¦å™¨ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
        if scheduler is not None:
            scheduler_path = save_dir / "scheduler.pt"
            torch.save(scheduler.state_dict(), scheduler_path)
            print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨å·²ä¿å­˜")
        
        # 4. ä¿å­˜éšæœºæ•°çŠ¶æ€
        rng_state = save_rng_state()
        rng_path = save_dir / "rng_state.pt"
        torch.save(rng_state, rng_path)
        print("âœ… éšæœºæ•°çŠ¶æ€å·²ä¿å­˜")
        
        # 5. ä¿å­˜è®­ç»ƒçŠ¶æ€
        training_state = {
            'epoch': epoch,
            'global_step': global_step,
            'data_index': data_index,
            'learning_rate': optimizer.param_groups[0]['lr'],
        }
        
        # æ·»åŠ é¢å¤–çŠ¶æ€
        if additional_state:
            training_state.update(additional_state)
            
        state_path = save_dir / "training_state.json"
        with open(state_path, 'w', encoding='utf-8') as f:
            json.dump(training_state, f, indent=2)
        print("âœ… è®­ç»ƒçŠ¶æ€å·²ä¿å­˜")
        
        # 6. ä¿å­˜replay bufferï¼ˆå¯é€‰ï¼‰
        if replay_buffer is not None and hasattr(replay_buffer, '__len__') and len(replay_buffer) > 0:
            buffer_path = save_dir / "replay_buffer.pt"
            torch.save(replay_buffer, buffer_path)
            print("âœ… Replay bufferå·²ä¿å­˜")
        
        print(f"ğŸ’¾ æ£€æŸ¥ç‚¹ä¿å­˜æˆåŠŸ: {save_dir} (epoch={epoch}, step={global_step}, data_index={data_index})")
        return True
        
    except Exception as e:
        print(f"[ERROR] ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return False


def load_checkpoint_complete(
    checkpoint_dir: str, 
    model, 
    tokenizer, 
    optimizer, 
    scheduler=None,
    total_data_len=None
):
    """å®Œæ•´çš„æ£€æŸ¥ç‚¹åŠ è½½å‡½æ•°"""
    checkpoint_path = Path(checkpoint_dir)
    if not checkpoint_path.exists():
        print(f"[ERROR] æ£€æŸ¥ç‚¹ä¸å­˜åœ¨: {checkpoint_path}")
        return 0, 0, 0, model, tokenizer, None
    
    try:
        print(f"[INFO] ä»æ£€æŸ¥ç‚¹æ¢å¤: {checkpoint_path}")
        
        # 1. åŠ è½½æ¨¡å‹
        model = PeftModel.from_pretrained(model, checkpoint_path)
        tokenizer = AutoTokenizer.from_pretrained(checkpoint_path)
        print("âœ… æ¨¡å‹å’Œtokenizerå·²åŠ è½½")
        
        # 2. åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        optimizer_path = checkpoint_path / "optimizer.pt"
        if optimizer_path.exists():
            optimizer.load_state_dict(torch.load(optimizer_path))
            print("âœ… ä¼˜åŒ–å™¨çŠ¶æ€å·²æ¢å¤")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°ä¼˜åŒ–å™¨çŠ¶æ€ï¼Œå°†ä½¿ç”¨åˆå§‹çŠ¶æ€")
        
        # 3. åŠ è½½å­¦ä¹ ç‡è°ƒåº¦å™¨
        if scheduler is not None:
            scheduler_path = checkpoint_path / "scheduler.pt"
            if scheduler_path.exists():
                scheduler.load_state_dict(torch.load(scheduler_path))
                print("âœ… å­¦ä¹ ç‡è°ƒåº¦å™¨å·²æ¢å¤")
            else:
                print("âš ï¸ æœªæ‰¾åˆ°è°ƒåº¦å™¨çŠ¶æ€")
        
        # 4. åŠ è½½éšæœºæ•°çŠ¶æ€
        rng_path = checkpoint_path / "rng_state.pt"
        if rng_path.exists():
            rng_state = torch.load(rng_path)
            load_rng_state(rng_state)
            print("âœ… éšæœºæ•°çŠ¶æ€å·²æ¢å¤ï¼Œç¡®ä¿ç”Ÿæˆæ•°æ®çš„ä¸€è‡´æ€§")
        else:
            print("âš ï¸ æœªæ‰¾åˆ°éšæœºæ•°çŠ¶æ€ï¼Œéšæœºæ€§å¯èƒ½ä¸ä¸€è‡´")
        
        # 5. åŠ è½½è®­ç»ƒçŠ¶æ€
        state_path = checkpoint_path / "training_state.json"
        if state_path.exists():
            with open(state_path, 'r', encoding='utf-8') as f:
                training_state = json.load(f)
            
            epoch = training_state.get('epoch', 0)
            global_step = training_state.get('global_step', 0)
            data_index = training_state.get('data_index', 0)
            learning_rate = training_state.get('learning_rate', None)
            
            if learning_rate and optimizer:
                for param_group in optimizer.param_groups:
                    param_group['lr'] = learning_rate
                print(f"âœ… å­¦ä¹ ç‡å·²æ¢å¤: {learning_rate}")
            
            print(f"âœ… è®­ç»ƒçŠ¶æ€å·²æ¢å¤: epoch={epoch}, global_step={global_step}, data_index={data_index}")
        else:
            # ä»ç›®å½•åæ¨æ–­
            if 'step_' in checkpoint_path.name:
                global_step = int(checkpoint_path.name.split('step_')[1])
                epoch = 0  # æ— æ³•å‡†ç¡®æ¨æ–­ï¼Œä»0å¼€å§‹
                data_index = 0
            else:
                epoch, global_step, data_index = 0, 0, 0
            print(f"âš ï¸ ä»ç›®å½•åæ¨æ–­çŠ¶æ€: global_step={global_step}")
        
        # 6. åŠ è½½replay bufferï¼ˆå¯é€‰ï¼‰
        buffer_path = checkpoint_path / "replay_buffer.pt"
        replay_buffer = None
        if buffer_path.exists():
            try:
                replay_buffer = torch.load(buffer_path)
                print("âœ… Replay bufferå·²æ¢å¤")
            except Exception as e:
                print(f"âš ï¸ Replay bufferåŠ è½½å¤±è´¥: {e}")
        
        return epoch, global_step, data_index, model, tokenizer, replay_buffer
        
    except Exception as e:
        print(f"[ERROR] åŠ è½½æ£€æŸ¥ç‚¹å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return 0, 0, 0, model, tokenizer, None


def verify_checkpoint(checkpoint_dir: str):
    """éªŒè¯æ£€æŸ¥ç‚¹çš„å®Œæ•´æ€§"""
    checkpoint_path = Path(checkpoint_dir)
    if not checkpoint_path.exists():
        return False, "æ£€æŸ¥ç‚¹ç›®å½•ä¸å­˜åœ¨"
    
    required_files = [
        "config.json",
        "adapter_model.bin",  # LoRAæƒé‡
        "optimizer.pt",
        "training_state.json"
    ]
    
    optional_files = [
        "scheduler.pt",
        "rng_state.pt",
        "replay_buffer.pt"
    ]
    
    missing_required = []
    missing_optional = []
    
    for file in required_files:
        if not (checkpoint_path / file).exists():
            missing_required.append(file)
    
    for file in optional_files:
        if not (checkpoint_path / file).exists():
            missing_optional.append(file)
    
    if missing_required:
        return False, f"ç¼ºå°‘å¿…éœ€æ–‡ä»¶: {missing_required}"
    
    status = "æ£€æŸ¥ç‚¹å®Œæ•´"
    if missing_optional:
        status += f"ï¼Œç¼ºå°‘å¯é€‰æ–‡ä»¶: {missing_optional}"
    
    return True, status


def load_whisper(model_name="openai/whisper-base", device="cuda:4"):
    """åŠ è½½Whisperæ¨¡å‹"""
    print(f"[INFO] åŠ è½½Whisperæ¨¡å‹: {model_name}, è®¾å¤‡: {device}")
    processor = WhisperProcessor.from_pretrained(model_name)
    model = WhisperForConditionalGeneration.from_pretrained(model_name).to(device)
    model.eval()
    print(f"[INFO] Whisperæ¨¡å‹åŠ è½½å®Œæˆ")
    return processor, model


def load_model(
    model_name_or_path: str,
    trust_remote_code: bool = True,
    bf16: bool = True,
    device_map=None,
    use_lora: bool = False,
    lora_config: Optional[LoraConfig] = None,
) -> tuple[MiniCPMO, PreTrainedTokenizer]:
    """åŠ è½½MiniCPMæ¨¡å‹"""
    from peft import get_peft_model
    print(f"[INFO] åŠ è½½MiniCPMæ¨¡å‹: {model_name_or_path}")
    tokenizer = AutoTokenizer.from_pretrained(model_name_or_path, trust_remote_code=trust_remote_code)
    tokenizer.pad_token = tokenizer.eos_token
    model = MiniCPMO.from_pretrained(
        model_name_or_path,
        trust_remote_code=trust_remote_code,
        attn_implementation='sdpa',
        torch_dtype=torch.float16
    )
    if use_lora:
        assert lora_config is not None, "LoRA config required"
        model = get_peft_model(model, lora_config)
        print(f"[INFO] LoRAé…ç½®åº”ç”¨å®Œæˆ")
    return model, tokenizer


def load_data_from_json(json_file_path: str) -> list:
    """ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®å¹¶æ„å»ºprompt"""
    prompts = []
    try:
        print(f"[INFO] ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®: {json_file_path}")
        with open(json_file_path, 'r', encoding='utf-8') as f:
            data = json.load(f)
        
        for item in data:
            text_id = item["id"]
            text_content = item["text"]
            labels = item["labels"]
            
            # è§£ææ ‡ç­¾ - æ”¹è¿›æ­£åˆ™è¡¨è¾¾å¼
            structure = re.search(r"1\.\s*Structure:\s*(.*?)(?:\n|$)", labels, re.MULTILINE)
            emotion = re.search(r"2\.\s*Emotion:\s*(.*?)(?:\n|$)", labels, re.MULTILINE)
            speech_speed = re.search(r"3\.\s*Speech Speed:\s*(.*?)(?:\n|$)", labels, re.MULTILINE)
            tone = re.search(r"4\.\s*Tone:\s*(.*?)(?:\n|$)", labels, re.MULTILINE)
            
            structure = structure.group(1).strip() if structure else ""
            emotion = emotion.group(1).strip() if emotion else ""
            speech_speed = speech_speed.group(1).strip() if speech_speed else ""
            tone = tone.group(1).strip() if tone else ""
            
            # æ„å»ºæŒ‡ä»¤
            instruction = f"Please express the sentence '{text_content}' with a structure that is {structure}, " \
                          f"emotionally {emotion}, at a speech speed of {speech_speed}, and in a {tone} tone, " \
                          f"voiced by a middle-aged woman."

            prompts.append({
                "prompt": instruction,
                "id": text_id,
                "original_text": text_content,
                "target_labels": {
                    "structure": structure.lower(),
                    "emotion": emotion.lower(),
                    "speech_speed": speech_speed.lower().replace(" ", "_"),
                    "tone": tone.lower()
                }
            })
            
        print(f"[INFO] ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®å®Œæˆï¼Œå…±{len(prompts)}æ¡è®°å½•")
        return prompts
        
    except Exception as e:
        print(f"[ERROR] ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return []


def pad_sequences_to_same_length(sequences: list, pad_token_id: int, max_length: int = None) -> torch.Tensor:
    """å°†åºåˆ—å¡«å……åˆ°ç›¸åŒé•¿åº¦"""
    if not sequences:
        return torch.empty(0)
    
    # å¦‚æœæ²¡æœ‰æŒ‡å®šæœ€å¤§é•¿åº¦ï¼Œä½¿ç”¨åºåˆ—ä¸­çš„æœ€å¤§é•¿åº¦
    if max_length is None:
        max_length = max(seq.size(1) for seq in sequences)
    
    padded_sequences = []
    for seq in sequences:
        current_length = seq.size(1)
        if current_length < max_length:
            # å³ä¾§å¡«å……
            padding = torch.full((seq.size(0), max_length - current_length), 
                               pad_token_id, dtype=seq.dtype, device=seq.device)
            padded_seq = torch.cat([seq, padding], dim=1)
        else:
            # æˆªæ–­åˆ°æœ€å¤§é•¿åº¦
            padded_seq = seq[:, :max_length]
        padded_sequences.append(padded_seq)
    
    return torch.cat(padded_sequences, dim=0)


@torch.no_grad()
def rollout_single(
    model: MiniCPMO,
    tokenizer: PreTrainedTokenizer,
    text: str,
    id: str,
    target_labels: dict,
    original_text: str,
    whisper_processor,
    whisper_model,
    qwen_analyzer: QwenAudioAnalyzer,
    wer_calculator: WERCalculator,
    num_rollouts: int,
    device: str,
    max_length: int = 1024,
    temperature: float = 1.0,
    top_p: float = 1.0,
) -> tuple[torch.Tensor, torch.Tensor, torch.Tensor, list[str]]:
    """å•ä¸ªæ ·æœ¬çš„rolloutç”Ÿæˆ"""

    model.eval()
    model = model.to(device)
    
    # æå–çº¯IDï¼ˆå»æ‰è·¯å¾„å’Œæ‰©å±•åï¼‰
    pure_id = os.path.splitext(os.path.basename(id))[0]

    chat_messages = [{'role':'user', 'content':[text]}]
    
    # ä¸ºæ¯ä¸ªrolloutç”Ÿæˆä¸åŒçš„éŸ³é¢‘æ–‡ä»¶å’Œè®¡ç®—å¥–åŠ±
    wers_list = []
    tag_rewards = []
    all_sequences = []
    all_action_masks = []
    all_results = []
    successful_rollouts = 0
    
    for i in range(num_rollouts):
        print(f"[INFO] å¤„ç† rollout {i+1}/{num_rollouts} for sample {pure_id}")
        
        # ç”Ÿæˆå”¯ä¸€çš„éŸ³é¢‘æ–‡ä»¶å
        audio_filename = f"{pure_id}_rollout_{i}_{uuid.uuid4().hex[:8]}.wav"
        audio_path = os.path.join(AUDIO_OUTPUT_DIR, audio_filename)
        
        # ä½¿ç”¨MiniCPMç”Ÿæˆè¯­éŸ³
        try:
            print(f"[DEBUG] å¼€å§‹ç”ŸæˆéŸ³é¢‘: {audio_path}")
            result = model.chat(
                msgs=chat_messages,
                tokenizer=tokenizer,
                sampling=True,
                max_new_tokens=128,
                use_tts_template=True,
                generate_audio=True,
                temperature=temperature,
                output_audio_path=audio_path,
            )
            
            print(f"[INFO] ç”ŸæˆéŸ³é¢‘æˆåŠŸ: {audio_path}")
            all_results.append(result)
            
            # å‡†å¤‡æ¨¡å‹è¾“å…¥ç”¨äºè·å–åºåˆ—
            model_inputs = model._prepare_inputs(
                msgs=chat_messages,
                tokenizer=tokenizer,
                sampling=True,
                use_tts_template=True,
                generate_audio=True,
                temperature=temperature,
                max_inp_length=256
            )
            
            pad_token_id = tokenizer.eos_token_id
            generation_config = GenerationConfig(
                do_sample=True,
                top_p=top_p,
                temperature=temperature,
                max_length=max_length,
                pad_token_id=pad_token_id,
            )
            
            _, outputs = model.generate(
                **model_inputs,
                generation_config=generation_config,
                tokenizer=tokenizer
            )
            
            # è·å–åºåˆ—ä¿¡æ¯
            sequence_ids = outputs.sequences
            action_mask = torch.ones_like(sequence_ids, dtype=torch.bool)
            action_mask[sequence_ids == pad_token_id] = False
            action_mask[sequence_ids == 0] = False
            action_mask = action_mask[:, 1:]
            
            all_sequences.append(sequence_ids)
            all_action_masks.append(action_mask)
            successful_rollouts += 1
            
            # ä½¿ç”¨Whisperè½¬å½•éŸ³é¢‘
            print(f"[DEBUG] å¼€å§‹Whisperè½¬å½•...")
            gen_text = wer_calculator.transcribe_with_whisper(
                audio_path, whisper_processor, whisper_model, device="cuda:4"
            )
            
            # è®¡ç®—WER
            print(f"[DEBUG] è®¡ç®—WER...")
            wer = wer_calculator.compute_wer(f"{pure_id}_rollout_{i}", original_text, gen_text)
            wers_list.append(torch.tensor([wer], dtype=torch.float))

            # è®¡ç®—æ ‡ç­¾åŒ¹é…å¥–åŠ±
            print(f"[DEBUG] è®¡ç®—æ ‡ç­¾åŒ¹é…å¥–åŠ±...")
            reward = qwen_analyzer.compute_tag_match_reward(audio_path, target_labels)
            tag_rewards.append(reward)
            
        except Exception as e:
            print(f"[ERROR] éŸ³é¢‘ç”Ÿæˆå¤±è´¥ rollout {i}: {e}")
            import traceback
            traceback.print_exc()
            continue

    if successful_rollouts == 0:
        print(f"[ERROR] æ‰€æœ‰rolloutéƒ½å¤±è´¥ï¼Œè·³è¿‡æ ·æœ¬ {pure_id}")
        return None, None, None, None

    # å¤„ç†ä¸è¶³çš„rollout - ä½¿ç”¨æœ€åä¸€ä¸ªæˆåŠŸçš„ç»“æœå¤åˆ¶
    if successful_rollouts < num_rollouts:
        print(f"[WARNING] åªæœ‰ {successful_rollouts}/{num_rollouts} rolloutæˆåŠŸï¼Œä½¿ç”¨æœ€åæˆåŠŸçš„ç»“æœå¡«å……")
        
        # è·å–æœ€åä¸€ä¸ªæˆåŠŸçš„ç»“æœ
        last_sequence = all_sequences[-1]
        last_action_mask = all_action_masks[-1]
        last_wer = wers_list[-1]
        last_reward = tag_rewards[-1]
        
        # å¤åˆ¶ç›´åˆ°è¾¾åˆ°æ‰€éœ€çš„rolloutæ•°é‡
        while len(all_sequences) < num_rollouts:
            all_sequences.append(last_sequence.clone())
            all_action_masks.append(last_action_mask.clone())
            wers_list.append(last_wer.clone())
            tag_rewards.append(last_reward)

    # å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦
    print(f"[DEBUG] å¡«å……åºåˆ—åˆ°ç›¸åŒé•¿åº¦...")
    try:
        # å…ˆæ£€æŸ¥åºåˆ—é•¿åº¦
        seq_lengths = [seq.size(1) for seq in all_sequences[:num_rollouts]]
        print(f"[DEBUG] åºåˆ—é•¿åº¦: {seq_lengths}")
        
        # ä½¿ç”¨å¡«å……å‡½æ•°
        sequence_ids = pad_sequences_to_same_length(
            all_sequences[:num_rollouts], 
            tokenizer.eos_token_id
        )
        
        # å¯¹action_maskä¹Ÿè¿›è¡Œç›¸åŒçš„å¡«å……å¤„ç†
        action_mask = pad_sequences_to_same_length(
            all_action_masks[:num_rollouts], 
            False  # action_maskç”¨Falseå¡«å……
        )
        
        print(f"[DEBUG] å¡«å……ååºåˆ—å½¢çŠ¶: {sequence_ids.shape}")
        print(f"[DEBUG] å¡«å……åaction_maskå½¢çŠ¶: {action_mask.shape}")
        
    except Exception as e:
        print(f"[ERROR] åºåˆ—å¡«å……å¤±è´¥: {e}")
        import traceback
        traceback.print_exc()
        return None, None, None, None

    # è®¡ç®—æœ€ç»ˆå¥–åŠ±
    wers_ = torch.stack(wers_list[:num_rollouts]).view(-1)
    returns = compute_reward(wers_, tag_rewards[:num_rollouts]).view(-1)

    # è®°å½•åˆ°wandb
    wandb.log({
        "reward_mean": returns.mean().item(),
        "reward_max": returns.max().item(),
        "reward_min": returns.min().item(),
        "WER_mean": wers_.mean().item(),
        "tag_reward_mean": sum(tag_rewards[:num_rollouts]) / len(tag_rewards[:num_rollouts])
    })

    print(f"[INFO] æ ·æœ¬ {pure_id} ç»“æœ:")
    print(f"[INFO]   WER scores: {wers_}")
    print(f"[INFO]   Tag rewards: {tag_rewards[:num_rollouts]}")
    print(f"[INFO]   Final rewards: {returns}")

    return sequence_ids, returns, action_mask, all_results[0] if all_results else None


def compute_reward(wer_scores, tag_rewards):
    """è®¡ç®—ç»¼åˆå¥–åŠ±"""
    wer_complement = 1.0 - wer_scores  # WER è¡¥å€¼ä¸º (1 - wer)ï¼Œå³æ­£ç¡®ç‡   

    rewards = 0.3 * wer_complement  # WERæƒé‡30%

    if tag_rewards is not None:
        tag_rewards_tensor = torch.tensor(tag_rewards, dtype=torch.float, device=wer_scores.device)
        rewards += 0.7 * tag_rewards_tensor  # æ ‡ç­¾åŒ¹é…æƒé‡70%

    rewards = torch.clamp(rewards, min=0.0, max=1.0)
    return rewards


def group_advantages(returns: torch.Tensor, eps: float = 1e-8) -> torch.Tensor:
    """è®¡ç®—ç¾¤ç»„ä¼˜åŠ¿"""
    return (returns - returns.mean()) / (returns.std() + eps)


def sequence_log_probs_from_logits(
    logits: torch.tensor, output_ids: torch.tensor
) -> torch.Tensor:
    """ä»logitsè®¡ç®—åºåˆ—logæ¦‚ç‡"""
    log_prob = F.log_softmax(logits, dim=-1)
    return log_prob.gather(dim=-1, index=output_ids.unsqueeze(-1)).squeeze(-1)


def sequences_log_probs(
    model: MiniCPMO,
    sequence_ids: torch.Tensor,
    attention_mask: torch.Tensor,
) -> torch.Tensor:
    """è®¡ç®—åºåˆ—çš„logæ¦‚ç‡"""
    device = attention_mask.device
    position_ids = attention_mask.long().cumsum(dim=-1) - 1
    position_ids = position_ids.to(device)
    position_ids.masked_fill_(mask=(attention_mask == 0), value=1)

    output = model.llm.forward(
        input_ids=sequence_ids,
        attention_mask=attention_mask,
        position_ids=position_ids,
        use_cache=False,
    )

    logits = output["logits"]
    log_probs = sequence_log_probs_from_logits(
        logits=logits[:, :-1].to(torch.float32),
        output_ids=sequence_ids[:, 1:],
    )
    return log_probs


def read_jsonl(file_name: str | Path) -> Iterator:
    """è¯»å–JSONLæ–‡ä»¶"""
    file_path = Path(file_name)
    with file_path.open(mode="r", encoding="utf-8") as f:
        for line in f:
            yield json.loads(line)

 
def main():
    print("ğŸš€ å¯åŠ¨MiniCPM GRPOè®­ç»ƒ...")
    
    # è®¾å¤‡é…ç½®
    model_device = torch.device("cuda:6")       # MiniCPMä¸»æ¨¡å‹
    ref_device = torch.device("cuda:7")         # MiniCPMå‚è€ƒæ¨¡å‹  
    whisper_device = torch.device("cuda:4")     # Whisperæ¨¡å‹
    # qwen_device = cuda:0 åœ¨qwen_audio_service.pyä¸­è®¾ç½®

    # è®­ç»ƒå‚æ•°
    num_epochs = 10
    seed = 42
    wandb_project = "None"  # è®¾ç½®wandbé¡¹ç›®å
    model_name = "MiniCPM-o"

    # LoRA é…ç½®
    lora_target_modules = []
    for i in range(28):
        for proj in ["q_proj", "k_proj", "v_proj", "o_proj"]:
            lora_target_modules.append(f"llm.model.layers.{i}.self_attn.{proj}")
        for proj in ["gate_proj", "up_proj", "down_proj"]:
            lora_target_modules.append(f"llm.model.layers.{i}.mlp.{proj}")

    lora_config = LoraConfig(
        r=8,
        lora_alpha=32,
        target_modules=lora_target_modules,
        lora_dropout=0.05,
        bias="none",
        task_type=TaskType.CAUSAL_LM
    )

    # å…¶ä»–è®­ç»ƒå‚æ•°
    checkpoint_path = Path("the path of checkpoint")
    checkpoint_path.mkdir(parents=True, exist_ok=True)
    
    checkpoint_interval = 20
    train_batch_size = 1
    lr = 5e-6
    kl_weight = 0.01
    clip_eps = 0.2
    group_size = 4
    epochs_per_step = 1
    max_norm = 1.0
    max_length = 256
    top_p = 1.0
    temperature = 1.0

    # ğŸ”¥ æ¢å¤é€‰é¡¹
    resume_from_checkpoint = "Set the checkpoint path to be restored"  # è®¾ç½®è¦æ¢å¤çš„æ£€æŸ¥ç‚¹è·¯å¾„
    # resume_from_checkpoint = None  # å¦‚æœè¦ä»å¤´å¼€å§‹è®­ç»ƒ

    # éªŒè¯æ£€æŸ¥ç‚¹ï¼ˆå¦‚æœå­˜åœ¨ï¼‰
    if resume_from_checkpoint:
        is_valid, status = verify_checkpoint(resume_from_checkpoint)
        if is_valid:
            print(f"âœ… æ£€æŸ¥ç‚¹éªŒè¯é€šè¿‡: {status}")
        else:
            print(f"âŒ æ£€æŸ¥ç‚¹éªŒè¯å¤±è´¥: {status}")
            print("å°†ä»å¤´å¼€å§‹è®­ç»ƒ...")
            resume_from_checkpoint = None

    print(f"[INFO] åˆå§‹åŒ–ç¡®å®šæ€§éšæœºæ•°ç§å­: {seed}")
    init_deterministic_rng(seed)

    print("ğŸ”§ åˆå§‹åŒ–åˆ†æå™¨å’Œè®¡ç®—å™¨...")
    
    # åˆå§‹åŒ–åˆ†æå™¨
    qwen_analyzer = QwenAudioAnalyzer()  # ä½¿ç”¨subprocessç‰ˆæœ¬
    wer_calculator = WERCalculator()

    # åŠ è½½Whisperæ¨¡å‹
    print(f"[INFO] åŠ è½½Whisperæ¨¡å‹åˆ°è®¾å¤‡: {whisper_device}")
    whisper_processor, whisper_model = load_whisper("the path of whisper-large-v3", device=whisper_device)
    print(f"âœ… Whisperæ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½å‚è€ƒæ¨¡å‹
    print(f"[INFO] åŠ è½½å‚è€ƒæ¨¡å‹åˆ°è®¾å¤‡: {ref_device}")
    reference_model, _ = load_model(model_name)
    reference_model.to(ref_device) 
    reference_model.eval()
    print(f"âœ… å‚è€ƒæ¨¡å‹åŠ è½½å®Œæˆ")

    # åŠ è½½è®­ç»ƒæ¨¡å‹
    print(f"[INFO] åŠ è½½è®­ç»ƒæ¨¡å‹åˆ°è®¾å¤‡: {model_device}")
    model, tokenizer = load_model(
        model_name,
        use_lora=True,
        lora_config=lora_config
    )
    model.to(model_device)
    model.print_trainable_parameters()
    model.init_tts()
    model.tts.float()
    print(f"âœ… è®­ç»ƒæ¨¡å‹åŠ è½½å®Œæˆ")

    # åˆ›å»ºä¼˜åŒ–å™¨å’Œå­¦ä¹ ç‡è°ƒåº¦å™¨
    optimizer = optim.Adam(model.parameters(), lr=lr)
    scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=100, gamma=0.95)  # æ¯100æ­¥è¡°å‡å­¦ä¹ ç‡
    
    # æ•°æ®åŠ è½½
    print("ğŸ“Š åŠ è½½è®­ç»ƒæ•°æ®...")
    pad_token_id = tokenizer.eos_token_id
    
    # ç›´æ¥ä»JSONæ–‡ä»¶åŠ è½½æ•°æ®
    json_path = "the path of processed_labels_3_formatted.json"
    prompts = load_data_from_json(json_path)
    
    if not prompts:
        print("âŒ æ•°æ®åŠ è½½å¤±è´¥ï¼Œé€€å‡ºç¨‹åº")
        return
    
    print(f"âœ… æ•°æ®åŠ è½½å®Œæˆï¼Œå…± {len(prompts)} ä¸ªæ ·æœ¬")
    
    # åˆ›å»ºæœ‰åºæ•°æ®è¿­ä»£å™¨
    data_iterator = OrderedDataIterator(prompts)
    
    # ğŸ”¥ æ¢å¤æ£€æŸ¥ç‚¹
    start_epoch = 0
    global_step = 0
    start_data_index = 0
    old_replay_buffer = None
    
    if resume_from_checkpoint:
        result = load_checkpoint_complete(
            resume_from_checkpoint, 
            model, 
            tokenizer, 
            optimizer, 
            scheduler,
            len(prompts)
        )
        start_epoch, global_step, start_data_index, model, tokenizer, old_replay_buffer = result
        
        # è®¾ç½®æ•°æ®è¿­ä»£å™¨çš„èµ·å§‹ä½ç½®
        data_iterator.set_current_index(start_data_index)
        print(f"âœ… æ•°æ®è¿­ä»£å™¨å·²è®¾ç½®åˆ°ç´¢å¼•: {start_data_index}")

    model.gradient_checkpointing_enable(
        gradient_checkpointing_kwargs={"use_reentrant": False}
    )

    replay_buffer = ReplayBuffer()
    if old_replay_buffer is not None:
        # å¯ä»¥é€‰æ‹©æ˜¯å¦ä½¿ç”¨æ—§çš„replay buffer
        print("[INFO] å‘ç°æ—§çš„replay bufferï¼Œé‡æ–°å¼€å§‹buffer")
        # replay_buffer = old_replay_buffer  # å¦‚æœè¦ä½¿ç”¨æ—§bufferï¼Œå–æ¶ˆæ³¨é‡Šè¿™è¡Œ
    
    objective = GRPOLoss(clip_eps=clip_eps, kl_weight=kl_weight)

    # åˆå§‹åŒ–wandb
    wandb.init(mode="disabled")
    print("[INFO] WandBå·²ç¦ç”¨")

    print("ğŸ¯ å¼€å§‹è®­ç»ƒå¾ªç¯...")
    print(f"[INFO] ä»epoch {start_epoch}, global_step {global_step}, data_index {start_data_index}å¼€å§‹")
    
    for epoch in range(start_epoch, num_epochs):
        print(f"\n{'='*60}")
        print(f"ğŸ”„ Epoch {epoch + 1}/{num_epochs}")
        print(f"{'='*60}")

        epoch_rollout_returns = []
        samples_processed_this_epoch = 0

        # æ¯ä¸ªepoché‡ç½®æ•°æ®è¿­ä»£å™¨ï¼ˆé™¤äº†ç¬¬ä¸€ä¸ªæ¢å¤çš„epochï¼‰
        if epoch > start_epoch:
            data_iterator.reset()
            print(f"[INFO] Epoch {epoch + 1}: æ•°æ®è¿­ä»£å™¨å·²é‡ç½®")

        # æŒ‰é¡ºåºå¤„ç†æ•°æ®
        try:
            while not data_iterator.is_finished():
                sample_data, current_data_index = next(data_iterator)
                
                print(f"\n{'â”€'*40}")
                print(f"ğŸ“ Epoch {epoch+1}, Global Step: {global_step}")
                print(f"ğŸ“ Data Index: {current_data_index}/{len(prompts)}")
                print(f"{'â”€'*40}")
                
                sample = sample_data
                texts = [sample["prompt"]]
                ids = [sample["id"]]
                target_labels_list = [sample["target_labels"]]
                original_texts = [sample["original_text"]]

                print(f"[INFO] å¤„ç†æ ·æœ¬: {ids[0]}")
                print(f"[INFO] æŒ‡ä»¤: {texts[0][:100]}...")
                print(f"[INFO] ç›®æ ‡æ ‡ç­¾: {target_labels_list[0]}")

                rollout_returns = []
                replay_buffer.clear()

                # Rollouté˜¶æ®µ
                print(f"[INFO] å¼€å§‹Rollouté˜¶æ®µ...")
                with torch.no_grad():
                    rollout_result = rollout_single(
                        model.to(model_device),
                        tokenizer,
                        text=texts[0],
                        id=ids[0],
                        target_labels=target_labels_list[0],
                        original_text=original_texts[0],
                        whisper_processor=whisper_processor,
                        whisper_model=whisper_model,
                        qwen_analyzer=qwen_analyzer,
                        wer_calculator=wer_calculator,
                        num_rollouts=group_size,
                        device=model_device,
                        max_length=max_length,
                        temperature=temperature,
                        top_p=top_p,
                    )

                    if rollout_result[0] is None:
                        print("âš ï¸  Rolloutå¤±è´¥ï¼Œè·³è¿‡æ­¤æ ·æœ¬")
                        global_step += 1
                        continue

                    sequence_ids, returns, action_mask, completions = rollout_result

                    sequence_ids = sequence_ids.to(model_device)
                    returns = returns.to(model_device)
                    action_mask = action_mask.to(model_device)

                    rollout_returns.append(returns.cpu())
                    epoch_rollout_returns.append(returns.cpu())
                    
                    advantages = group_advantages(returns).view(-1).to(model_device)
                    
                    if torch.all(advantages == 0):
                        print("âš ï¸  æ‰€æœ‰advantagesä¸º0ï¼Œè·³è¿‡æ­¤batch")
                        global_step += 1
                        continue
                        
                    returns = returns.view(-1) 
                    attention_mask = sequence_ids != torch.full_like(sequence_ids, pad_token_id, device=sequence_ids.device)

                    print("[INFO] è®¡ç®—log probabilities...")
                    log_probs = sequences_log_probs(
                        model=model,
                        sequence_ids=sequence_ids.to(model_device),
                        attention_mask=attention_mask.to(model_device),
                    )
                    log_probs_ref = sequences_log_probs(
                        model=reference_model,
                        sequence_ids=sequence_ids.to(ref_device),
                        attention_mask=attention_mask.to(ref_device),
                    )
                    kl = approx_kl_divergence(
                        log_probs=log_probs,
                        log_probs_ref=log_probs_ref.to(log_probs.device),
                        action_mask=action_mask,
                    )

                    experience = Experience(
                        sequences=sequence_ids,
                        action_log_probs=log_probs,
                        log_probs_ref=log_probs_ref,
                        returns=returns,
                        advantages=advantages,
                        attention_mask=attention_mask,
                        action_mask=action_mask,
                        kl=kl,
                    )
                    replay_buffer.append(experience)

                torch.cuda.empty_cache()
                
                if rollout_returns:
                    episode_return_sum = torch.stack(rollout_returns).sum()
                    print(f"[INFO] Step {global_step} returns: {episode_return_sum:.4f}")
                    wandb.log({"returns": episode_return_sum, "step": global_step})
                else:
                    print("âš ï¸  æ— æœ‰æ•ˆreturns")
                    global_step += 1
                    continue

                # ç­–ç•¥æ›´æ–°é˜¶æ®µ
                print("[INFO] å¼€å§‹ç­–ç•¥æ›´æ–°...")
                experience_sampler = DataLoader(
                    replay_buffer,
                    batch_size=train_batch_size,
                    shuffle=True,
                    drop_last=True,
                    collate_fn=join_experience_batch,
                )

                for step_epoch in range(epochs_per_step):
                    model.train()
                    total_loss = 0.0
                    total_steps = 0

                    for exp in experience_sampler:
                        exp: Experience
                        exp = exp.to(next(model.parameters()).device)
                        optimizer.zero_grad()

                        log_probs = sequences_log_probs(
                            model, sequence_ids=exp.sequences, attention_mask=exp.attention_mask
                        )

                        loss, kl = objective(log_probs=log_probs, experience=exp)
                        
                        if not loss.isfinite() or not kl.isfinite():
                            print(f"âš ï¸  Loss not finite, skipping: loss={loss}, kl={kl}")
                            continue

                        loss.backward()
                        total_loss += loss.item()
                        total_steps += 1
                        
                        grad_norm = clip_grad_norm_(model.parameters(), max_norm=max_norm)
                        
                        wandb.log({
                            "loss": loss.item(),
                            "kl": kl.item(), 
                            "grad_norm": grad_norm.item(),
                            "learning_rate": optimizer.param_groups[0]['lr'],
                            "step": global_step
                        })

                        optimizer.step()
                        
                    if total_steps > 0:
                        avg_loss = total_loss / total_steps
                        print(f"[INFO] Epoch {step_epoch+1} avg loss: {avg_loss:.6f}")
                    else:
                        avg_loss = 0.0
                        print("âš ï¸  No valid training steps")

                # æ›´æ–°å­¦ä¹ ç‡è°ƒåº¦å™¨
                scheduler.step()
                
                # ğŸ”¥ ä¿å­˜æ£€æŸ¥ç‚¹ï¼ˆä½¿ç”¨å®Œæ•´ç‰ˆæœ¬ï¼‰
                if (
                    checkpoint_path is not None
                    and checkpoint_interval is not None
                    and (global_step + 1) % checkpoint_interval == 0
                ):
                    additional_state = {
                        'samples_processed_this_epoch': samples_processed_this_epoch + 1,
                        'total_samples': len(prompts),
                    }
                    
                    success = save_checkpoint_complete(
                        checkpoint_path, 
                        global_step, 
                        epoch, 
                        current_data_index + 1,  # ä¸‹ä¸€ä¸ªè¦å¤„ç†çš„æ•°æ®ç´¢å¼•
                        model, 
                        tokenizer, 
                        optimizer, 
                        scheduler,
                        replay_buffer,
                        additional_state
                    )
                    
                    if not success:
                        print(f"[WARNING] ä¿å­˜æ£€æŸ¥ç‚¹å¤±è´¥ï¼Œä½†ç»§ç»­è®­ç»ƒ...")
                    
                global_step += 1
                samples_processed_this_epoch += 1
                
        except StopIteration:
            print(f"[INFO] Epoch {epoch + 1} æ‰€æœ‰æ•°æ®å¤„ç†å®Œæˆ")
            
        # Epochç»“æŸç»Ÿè®¡
        if epoch_rollout_returns:
            epoch_avg_return = torch.cat(epoch_rollout_returns).mean().item()
            print(f"\nğŸ Epoch {epoch+1} å®Œæˆ:")
            print(f"   å¤„ç†æ ·æœ¬æ•°: {samples_processed_this_epoch}")
            print(f"   å¹³å‡å¥–åŠ±: {epoch_avg_return:.4f}")
            print(f"   å½“å‰å­¦ä¹ ç‡: {scheduler.get_last_lr()[0]:.2e}")
            
            wandb.log({
                "epoch": epoch + 1,
                "epoch_avg_return": epoch_avg_return,
                "samples_processed": samples_processed_this_epoch,
                "learning_rate": scheduler.get_last_lr()[0],
            })
        else:
            print(f"\nâš ï¸  Epoch {epoch+1} æ— æœ‰æ•ˆæ•°æ®")

    # ä¿å­˜æœ€ç»ˆæ¨¡å‹
    if checkpoint_path is not None:
        final_dir = checkpoint_path / "final"
        model.save_pretrained(final_dir)
        tokenizer.save_pretrained(final_dir)
        
        # ä¿å­˜æœ€ç»ˆçŠ¶æ€
        final_state = {
            'completed_epochs': num_epochs,
            'total_steps': global_step,
            'final_learning_rate': optimizer.param_groups[0]['lr'],
            'training_completed': True
        }
        
        with open(final_dir / "final_state.json", 'w', encoding='utf-8') as f:
            json.dump(final_state, f, indent=2)
            
        print(f"ğŸ‰ æœ€ç»ˆæ¨¡å‹å·²ä¿å­˜: {final_dir}")

    print("\n" + "="*60)
    print("ğŸ‰ è®­ç»ƒå®Œæˆï¼")
    print("="*60)
    print(f"ğŸ“ è¾“å‡ºæ–‡ä»¶ä½ç½®:")
    print(f"   ğŸµ éŸ³é¢‘æ–‡ä»¶: {AUDIO_OUTPUT_DIR}")
    print(f"   ğŸ“Š WERç»“æœ: {WER_RESULT_CSV}")
    print(f"   ğŸ·ï¸  æ ‡ç­¾åˆ†æ: {AUDIO_LABEL_RESULT_JSON}")
    print(f"   ğŸ’¾ æ¨¡å‹æ£€æŸ¥ç‚¹: {checkpoint_path}")


if __name__ == "__main__":
    main()